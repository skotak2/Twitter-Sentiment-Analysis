{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNf1nNWmzcxBRdh0QPu33dX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skotak2/sentiment_analysis/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqNsK53GrEmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8f003642-13ee-45f7-e5e0-7c5ce9671e86"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import coo_matrix, hstack\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIYcYRbSrIUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "20c2f2a9-82a1-4d1a-cf97-c90675434e52"
      },
      "source": [
        "!pip install -q xlrd\n",
        "!git clone https://github.com/skotak2/sentiment_analysis.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sentiment_analysis'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbaD60zxus4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linelist1=[]\n",
        "linelist2=[]\n",
        "target1 = []\n",
        "target2 = []\n",
        "with open ('sentiment_analysis/train_566.txt',encoding=\"utf8\") as t1:\n",
        "    for line in t1:\n",
        "        linelist1.append(word_tokenize((line.split(',',4)[3].lstrip())))\n",
        "        target1.append(line.split(',', 4)[1])\n",
        "with open ('sentiment_analysis/test_566.txt',encoding=\"utf8\") as t2:\n",
        "    for line in t2:\n",
        "        linelist2.append(word_tokenize((line.split(',',4)[3].lstrip())))\n",
        "        target2.append(line.split(',', 4)[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81frGlpJyFHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_tweets_train = []\n",
        "list_stop_words = set(stopwords.words('english'))\n",
        "for i in range (1,len(linelist1)):\n",
        "    token = linelist1[i]\n",
        "    filtoken=[]\n",
        "    for word in token:\n",
        "        if word not in list_stop_words:\n",
        "            if word.isalpha():\n",
        "                filtoken.append(sno.stem(word.lower()))\n",
        "    all_tweets_train.append(filtoken)\n",
        "\n",
        "all_tweets_test = []\n",
        "for i in range (1,len(linelist2)):\n",
        "    token = linelist2[i]\n",
        "    filtoken=[]\n",
        "    for word in token:\n",
        "        if word not in list_stop_words:\n",
        "            if word.isalpha():\n",
        "                filtoken.append(sno.stem(word.lower()))\n",
        "    all_tweets_test.append(filtoken)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6MYLogEyWZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = all_tweets_train\n",
        "b = all_tweets_test\n",
        "lis1 = []\n",
        "lis2 = []\n",
        "def listToString(s):  \n",
        "    str1 = \" \"    \n",
        "    return (str1.join(s))              \n",
        "for ele in a:\n",
        "    strg = listToString(ele)\n",
        "    lis1.append(strg)\n",
        "for ele in b:\n",
        "    strg = listToString(ele)\n",
        "    lis2.append(strg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4u1ruFqyebS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "train_vectors = pd.DataFrame( data = vectorizer.fit_transform(lis1).todense(),columns = vectorizer.get_feature_names())\n",
        "test_vectors = pd.DataFrame( data = vectorizer.transform(lis2).todense(),columns = vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqP0N8nOymho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target1.pop(0)\n",
        "target2.pop(0)\n",
        "target1 = [float(i) for i in target1]\n",
        "target2 = [float(i) for i in target2]\n",
        "y_label = pd.DataFrame( data = target1)\n",
        "y_test = pd.DataFrame( data = target2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lCb_xG1yrsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, alp=0.01, niter=1000):\n",
        "        self.lr = alp\n",
        "        self.niter = niter\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))   #test.values.astype(float)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        no_of_cols = 10000#X.shape[1]\n",
        "        self.theta = np.zeros((no_of_cols,1))\n",
        "        for i in range(self.niter):\n",
        "            z = np.dot(X, self.theta)\n",
        "            y_out = self.sigmoid(z)\n",
        "            y_out = y_out.reshape(y.shape)\n",
        "            gradient = np.dot(X.T, (y_out - y)) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "            \n",
        "            if(i % 10000 == 0):\n",
        "                z = np.dot(X, self.theta)\n",
        "                y_out = self.sigmoid(z)\n",
        "\n",
        "    def predict_prob(self, X):\n",
        "        return self.sigmoid(np.dot(X, self.theta))\n",
        "    def predict(self, X, th):\n",
        "        label_array = np.array([])\n",
        "        pred = self.predict_prob(X)\n",
        "        \n",
        "        for i in pred:\n",
        "          if i > th:\n",
        "            label_array = np.append(label_array,1)\n",
        "          else:\n",
        "            label_array = np.append(label_array,0)\n",
        "        return label_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1mx9l1iywVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train_vectors.to_numpy()\n",
        "X_test = test_vectors.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i4MeyAuzDGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "4719d2d5-7943-4b14-f511-2a4c5338b6e3"
      },
      "source": [
        "kf = KFold(n_splits=10)\n",
        "sc = np.array([])\n",
        "count = 1\n",
        "for train_index, test_index in kf.split(train_vectors):\n",
        "  x_train, x_test = train_vectors.iloc[train_index],train_vectors.iloc[test_index]\n",
        "  y_train, y_test = y_label.iloc[train_index],y_label.iloc[test_index]\n",
        "  logreg_model = LogisticRegression()\n",
        "  print(\"Runing model number:\",count)\n",
        "  logreg_model.fit(x_train.to_numpy(),y_train.to_numpy())\n",
        "  pred = logreg_model.predict_prob(x_test.to_numpy())\n",
        "  th = sum(pred)/len(pred)\n",
        "  labels = logreg_model.predict(x_test,th)\n",
        "  print(\"completed!! model number:\",count)\n",
        "  sc = np.append(sc,accuracy_score(y_test,labels))\n",
        "  count +=1\n",
        "print(sc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runing model number: 1\n",
            "completed!! model number: 1\n",
            "Runing model number: 2\n",
            "completed!! model number: 2\n",
            "Runing model number: 3\n",
            "completed!! model number: 3\n",
            "Runing model number: 4\n",
            "completed!! model number: 4\n",
            "Runing model number: 5\n",
            "completed!! model number: 5\n",
            "Runing model number: 6\n",
            "completed!! model number: 6\n",
            "Runing model number: 7\n",
            "completed!! model number: 7\n",
            "Runing model number: 8\n",
            "completed!! model number: 8\n",
            "Runing model number: 9\n",
            "completed!! model number: 9\n",
            "Runing model number: 10\n",
            "completed!! model number: 10\n",
            "[0.69222222 0.64966667 0.62111111 0.61444444 0.62155556 0.61955556\n",
            " 0.61555556 0.61122222 0.62077778 0.61111111]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}